@article{DBLP:journals/corr/abs-1806-01261,
  author    = {Peter W. Battaglia and
               Jessica B. Hamrick and
               Victor Bapst and
               Alvaro Sanchez{-}Gonzalez and
               Vin{\'{\i}}cius Flores Zambaldi and
               Mateusz Malinowski and
               Andrea Tacchetti and
               David Raposo and
               Adam Santoro and
               Ryan Faulkner and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               H. Francis Song and
               Andrew J. Ballard and
               Justin Gilmer and
               George E. Dahl and
               Ashish Vaswani and
               Kelsey R. Allen and
               Charles Nash and
               Victoria Langston and
               Chris Dyer and
               Nicolas Heess and
               Daan Wierstra and
               Pushmeet Kohli and
               Matthew Botvinick and
               Oriol Vinyals and
               Yujia Li and
               Razvan Pascanu},
  title     = {Relational inductive biases, deep learning, and graph networks},
  journal   = {CoRR},
  volume    = {abs/1806.01261},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.01261},
  archivePrefix = {arXiv},
  eprint    = {1806.01261},
  timestamp = {Wed, 24 Jul 2019 18:56:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-01261.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
bojchevski2018deep,
title={Deep Gaussian Embedding of Graphs:  Unsupervised Inductive Learning via Ranking},
author={Aleksandar Bojchevski and Stephan Günnemann},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1ZdKJ-0W},
}

@inproceedings{DBLP:conf/icml/GilmerSRVD17,
  author={Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  title={Neural Message Passing for Quantum Chemistry},
  year={2017},
  cdate={1483228800000},
  pages={1263-1272},
  url={http://proceedings.mlr.press/v70/gilmer17a.html},
  booktitle={ICML},
  crossref={conf/icml/2017}
}

@inproceedings{DBLP:conf/nips/HamiltonYL17,
  author={William L. Hamilton and Zhitao Ying and Jure Leskovec},
  title={Inductive Representation Learning on Large Graphs},
  year={2017},
  cdate={1483228800000},
  pages={1025-1035},
  url={http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs},
  booktitle={NIPS},
  crossref={conf/nips/2017}
}

@article{Montavon_2013,
	doi = {10.1088/1367-2630/15/9/095003},
	url = {https://doi.org/10.1088%2F1367-2630%2F15%2F9%2F095003},
	year = 2013,
	month = {sep},
	publisher = {{IOP} Publishing},
	volume = {15},
	number = {9},
	pages = {095003},
	author = {Gr{\'{e}}goire Montavon and Matthias Rupp and Vivekanand Gobre and Alvaro Vazquez-Mayagoitia and Katja Hansen and Alexandre Tkatchenko and Klaus-Robert Müller and O Anatole von Lilienfeld},
	title = {Machine learning of molecular electronic properties in chemical compound space},
	journal = {New Journal of Physics},
	abstract = {The combination of modern scientific computing with electronic structure theory can lead to an unprecedented amount of data amenable to intelligent data analysis for the identification of meaningful, novel and predictive structure–property relationships. Such relationships enable high-throughput screening for relevant properties in an exponentially growing pool of virtual compounds that are synthetically accessible. Here, we present a machine learning model, trained on a database of ab initio calculation results for thousands of organic molecules, that simultaneously predicts multiple electronic ground- and excited-state properties. The properties include atomization energy, polarizability, frontier orbital eigenvalues, ionization potential, electron affinity and excitation energies. The machine learning model is based on a deep multi-task artificial neural network, exploiting the underlying correlations between various molecular properties. The input is identical to ab initio methods, i.e. nuclear charges and Cartesian coordinates of all atoms. For small organic molecules, the accuracy of such a ‘quantum machine’ is similar, and sometimes superior, to modern quantum-chemical methods—at negligible computational cost.}
}

@article{Sen2008,
    abstract = {Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.},
    author = {Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina},
    doi = {10.1609/aimag.v29i3.2157},
    file = {::},
    issn = {0738-4602},
    journal = {AI Magazine},
    mendeley-groups = {Deep learning/GNN},
    month = {sep},
    number = {3},
    pages = {93},
    title = {{Collective Classification in Network Data}},
    url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/2157},
    volume = {29},
    year = {2008}
}
